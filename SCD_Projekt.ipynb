{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Instanzieren der Klasse DatabaseMeta in der alle Metadaten der Datenbank gespeichert sind."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prüfung erfolgreich abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "from DatabaseMeta import DatabaseMeta\n",
    "dbm = DatabaseMeta(\"sqlite:///test.db\", verbose_mode=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instanzieren der Klasse ETLPipeline; dem Konstruktor wird das DatabaseMeta-Objekt übergeben."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from ETL import ETLPipeline\n",
    "etl = ETLPipeline(dbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um die Dimensionstabellen mit den Ausgangsdaten zu befüllen,\n",
    "werden zunächst mehrere csv-Dateien in ein Pandas-Dataframe eingelesen.\n",
    "Die Dateien liegen im Ordner 'dim_data' und\n",
    "beinhalten die Ausgangsdaten für die Dimensionstabellen 'DT_Produkt', 'DT_Personal' und 'DT_Kunde'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "produkte_col_types = {   \"kategorie\": str,\n",
    "                \"produkt_code\": str,\n",
    "                \"name\": str,\n",
    "                \"variante\": str,\n",
    "                \"hersteller_artikel_code\": str,\n",
    "                \"hersteller_code\": str,\n",
    "                \"hersteller_name\": str,\n",
    "                \"Verkaufspreis_Brutto\": str,\n",
    "                \"Verkaufspreis_MwSt\": str,\n",
    "                \"hersteller_plz\": str,\n",
    "                \"hersteller_ort\": str,\n",
    "                \"hersteller_land\": str}\n",
    "csv_file = './dim_data/produkte.csv'\n",
    "produkte = pd.read_csv(csv_file, dtype=produkte_col_types)\n",
    "produkte[\"Verkaufspreis_Brutto\"] = pd.to_numeric(produkte[\"Verkaufspreis_Brutto\"])\n",
    "produkte[\"Verkaufspreis_MwSt\"] = pd.to_numeric(produkte[\"Verkaufspreis_MwSt\"])\n",
    "produkte[\"verkaufspreis_netto\"] = produkte[\"Verkaufspreis_Brutto\"]-produkte[\"Verkaufspreis_MwSt\"]\n",
    "produkte[\"verkaufspreis_netto\"] = pd.to_numeric(produkte[\"verkaufspreis_netto\"])\n",
    "produkte[\"verkaufspreis_netto\"] = produkte[\"verkaufspreis_netto\"].round(2)\n",
    "produkte = produkte.drop(labels=[\"Verkaufspreis_Brutto\",\"Verkaufspreis_MwSt\"], axis=1)\n",
    "produkte = produkte.drop(labels=[\"datum\",\"uhrzeit\"], axis=1)\n",
    "\n",
    "kunden_col_types = {   \"kunde_code\": str,\n",
    "                \"name\": str,\n",
    "                \"vorname\": str,\n",
    "                \"firma\": str,\n",
    "                \"plz\": str,\n",
    "                \"ort\": str,\n",
    "                \"land\": str}\n",
    "csv_file = './dim_data/kunden.csv'\n",
    "kunden = pd.read_csv(csv_file, dtype=kunden_col_types)\n",
    "\n",
    "personal_col_types = {   \"personal_code\": str,\n",
    "                \"name\": str,\n",
    "                \"vorname\": str,\n",
    "                \"abteilung\": str,\n",
    "                \"abteilung_vorher\": str}\n",
    "csv_file = './dim_data/personal.csv'\n",
    "personal = pd.read_csv(csv_file, dtype=personal_col_types)\n",
    "personal = personal.drop(labels=[\"datum\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Im nächsten Schritt werden die DataFrames mit den Ausgangsdaten mittels der Funktion 'change_dim_value' einzeln\n",
    "der ETLPipeline übergeben. Die Pipeline speichert die Daten in der Datenbank.\n",
    "Der Gültigkeitsbeginn der Ausgangsdaten wird auf das frühestmögliche Datum gesetzt 'date.min' (dritter Parameter).\n",
    "\n",
    "In die Dimensionstabelle 'DT_Personal' sollen Werte in die Spalte 'abteilung_vorher' gespeichert werden.\n",
    "Diese Spalte ist für die Aufnahme von SCD-Typ 3 Werten markiert.\n",
    "Daher muss der optionale Parameter 'scd1' True gesetzt werden. Damit können in allen Spalten Werte überschrieben werden.\n",
    "\n",
    "Um die Änderungen zu Verfolgen sollte ein Tool der Wahl genutzt werden, um die (SQLite-)Datenbank auszulesen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py:724: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\n",
      "  util.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n",
      "ACHTUNG: Werte werden nun einfach überschrieben!\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "for i in range(produkte.shape[0]):\n",
    "    etl.change_dim_value('DT_Produkt', produkte.iloc[i].to_dict(), date.min)\n",
    "\n",
    "for i in range(kunden.shape[0]):\n",
    "    etl.change_dim_value('DT_Kunde', kunden.iloc[i].to_dict(), date.min)\n",
    "\n",
    "for i in range(personal.shape[0]):\n",
    "    etl.change_dim_value('DT_Personal', personal.iloc[i].to_dict(), date.min, scd1=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instanziere die Klasse TestDataGenerator. Dem Konstruktor ist ein dict zu übergeben, dass den Dimensionen eine\n",
    "csv-Datei zuweist, in der sich die Dimensionsdaten befinden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from TestDataGenerator import TestDataGenerator\n",
    "tdg = TestDataGenerator({'personal':'./test_data/personal_original.csv',\n",
    "                         'kunde':'./test_data/kunden_original.csv',\n",
    "                         'produkt':'./test_data/produkte_original.csv'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun können mit dem TestDataGenerator Faktendaten erzeugt werden.\n",
    "Die Methode 'dump_data' erzeugt eine csv-Datei mit den Testdaten. Als Parameter ist die Anzahl an Datenreihen anzugeben.\n",
    "Die Methode gibt den erzeugten Dateinamen zurück, die Datei wird im Ordner 'testData' abgespeichert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Code-Zelle TD1\n",
    "file_path = tdg.dump_data(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die csv-Datei wird der ETLPipeline übergeben. Die Fakten werden in der Datenbank gespeichert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code-Zelle TD2\n",
    "etl.etl_fact_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Code-Zellen TD1 und TD2 können nun beliebig oft wiederholt werden, um weitere Testdaten in die Datenbank zu laden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Im Folgenden werden csv-Dateien eingelesen, die Veränderungen (mit Zeitstempel) von Dimensionswerten enthalten."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In die Tabelle 'DT_Kunde' werden neue Datensätze eingefügt:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "etl.etl_dim_csv('./test_data/kunden_neu.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um die neuen Kunden bei der Generierung von weiteren Faktendaten zu berücksichtigen,\n",
    "wird die neue Datenbasis dem 'TestDataGenerator'-Objekt \"tdg\" übergeben.\n",
    "Dazu muss diese zuvor aus der Tabelle 'DT_Kunde' exportiert werden.\n",
    "Dafür wird die Helferfunktion \"dump_dim_data\" definiert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def dump_dim_data(table:str, file_name:str) -> str:\n",
    "    with etl._ETLPipeline__engine.connect() as conn:\n",
    "        dim_DF = pd.read_sql_table(table,conn)\n",
    "        dim_DF = dim_DF.drop('id',axis=1)  # die technische ID wird nicht benötigt\n",
    "        full_file_path = 'dim_data_drops/' + file_name + '.csv'\n",
    "        dim_DF.to_csv(full_file_path,index=False)\n",
    "        return full_file_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Funktion gibt den Dateipfad zur gespeicherten csv-Datei zurück.\n",
    "Dieser wird dann dem 'TestDataGenerator'-Objekt \"tdg\" übergeben."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py:724: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\n",
      "  util.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_file = dump_dim_data('DT_Kunde', 'DT_Kunde__neue_Kunden')\n",
    "tdg.change_data({'kunde': dump_file})\n",
    "file_path = tdg.dump_data(1000)\n",
    "etl.etl_fact_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SCD Typ 1:\n",
    "\n",
    "Bei einigen Kunden haben sich der Wohnort oder Nachname geändert.\n",
    "Die neuen Werte überschreiben die alten Werte in der Tabelle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "etl.etl_dim_csv('./test_data/kunden_changes.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Diese Änderungen müssen nicht dem 'TestDataGenerator'-Objekt \"tdg\" übergeben werden.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SCD Typ 2:\n",
    "\n",
    "Am 01.07.2020 trat die temporäre Absenkung der Mehrwertsteuer von 19 % auf 16 % in Kraft.\n",
    "Daher ändert sich ab diesem Tag der Verkaufspreis der Produkte.\n",
    "Außerdem ändern sich einige Variantenbezeichnungen und es kommen neue Produkte hinzu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "etl.etl_dim_csv('./test_data/produkte_changes.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Diese Daten werden wiederum dem 'TestDataGenerator'-Objekt \"tdg\" übergeben."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dump_file = dump_dim_data('DT_Produkt', 'DT_Produkt__aenderungen')\n",
    "tdg.change_data({'produkt': dump_file})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neu erzeugte Faktendaten können jedoch erst nach Beginn der Gültigkeit die neuen Datensätze referenzieren!\n",
    "Daher wird das Datum in den November 2020 geändert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datum geändert zu 2020-11-01T00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py:724: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\n",
      "  util.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "tdg.skip_to_date(datetime(2020,11,1))\n",
    "file_path = tdg.dump_data(1000)\n",
    "etl.etl_fact_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Besonderheit Star-Schema:\n",
    "\n",
    "In der Tabelle 'DT_Produkt' sind auch Herstellerdaten gespeichert.\n",
    "Die Tabelle ist lediglich in der ersten Normalform.\n",
    "Daher müssen Änderungen, die Herstellerdaten betreffen, in allen entsprechenden Datensätzen geändert werden.\n",
    "\n",
    "Beim Snowflake-Schema liegen die Tabellen in normalisierter Form vor. Daher tritt diese Besonderheit dort nicht auf."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "45"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl.change_dim_value('DT_Produkt',{'produkt_code': '445862-OW', 'hersteller_name': 'Squeeler'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bei der Änderung des Herstellernamens in einem Datensatz werden alle Datensätze geändert, die den gleichen\n",
    "'hersteller_code' enthalten.\n",
    "Diese Änderung führt hier nicht zu einer Historisierung (dies ließe sich jedoch so programmieren)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SCD Typ 3:\n",
    "\n",
    "Einige Mitarbeiter haben die Abteilung gewechselt.\n",
    "Die bisherige Abteilung wird in der Spalte 'abteilung_vorher' gespeichert.\n",
    "Die neue Abteilung wird in 'abteilung' gespeichert.\n",
    "Alte Werte werden von neuen Werten \"herausgeschoben\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n",
      "abteilung in DT_Personal zu abteilung_vorher\n"
     ]
    }
   ],
   "source": [
    "etl.etl_dim_csv('./test_data/personal_changes.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zusatz SCD Typ 3:\n",
    "\n",
    "Durch eine Zusammenführung von bisher getrennten\n",
    "Personalwirtschaftssystemen sind neue Personal-Code erforderlich geworden.\n",
    "Da der Zeitpunkt dieser Maßnahme bekannt ist und ein solcher Wechsel nicht wieder erwartet wird,\n",
    "jedoch die alten Personal-Code weiterhin gespeichert bleiben sollen, wird dies mit dem SCD Typ 3 abgebildet.\n",
    "\n",
    "Da die alten Personal-Code eineindeutig sind, identifizieren diese auch noch jeden Datensatz.\n",
    "Die erforderliche Mapping-Tabelle von alten zu neuen Personal-Code besteht also nur aus jeweils diesen beiden.\n",
    "\n",
    "In der Tabelle 'DT_Personal' wird die Spalte 'personal_code_neu' ergänzt.\n",
    "Da in diesem Prototyp noch keine Versionierung der Datenbank vorgenommen wird, werden nun reine SQL-Befehle genutzt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "with etl._ETLPipeline__engine.connect() as conn:\n",
    "    conn.execute('ALTER TABLE DT_Personal ADD personal_code_neu TEXT')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sollen die neuen Personal-Codes über die ETL-Pipeline eingepflegt werden, müsste zunächst das DatabaseMeta-Objekt angepasst werden.\n",
    "Hier nun der direkte Weg mit Hilfe des UPDATE-Befehl (sql)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with open('./test_data/personal_mapping.csv','r') as map_file:\n",
    "    map_df = pd.read_csv(map_file,dtype=str)\n",
    "    with etl._ETLPipeline__engine.connect() as conn:\n",
    "        for i in range(map_df.shape[0]):\n",
    "            stmt = text('UPDATE DT_Personal SET personal_code_neu = :x WHERE personal_code = :y')\n",
    "            stmt = stmt.bindparams(x = map_df.iloc[i][\"personal_code_neu\"], y = map_df.iloc[i][\"personal_code\"])\n",
    "            conn.execute(stmt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun wird die Spalte 'personal_code' in 'personal_code_alt' umbenannt\n",
    "und die Spalte 'personal_code_neu' in 'personal_code'.\n",
    "Das oben beschriebene Verhalten der Abteilungsspalten (herausschieben von neuen Daten) wird hier nicht implementiert,\n",
    "da die Personal-Code sich grundsätzlich nicht ändern."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with etl._ETLPipeline__engine.connect() as conn:\n",
    "    conn.execute('ALTER TABLE DT_Personal RENAME COLUMN personal_code TO personal_code_alt')\n",
    "    conn.execute('ALTER TABLE DT_Personal RENAME COLUMN personal_code_neu TO personal_code')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Diese Änderung im Tabellenschema wird nun noch der ETL-Pipeline mitgegeben.\n",
    "Dazu wird das DatabaseMeta-Objekt geändert."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, Column, Integer, String\n",
    "etl.dbm.DT_employee_table = Table(\"DT_Personal\", etl.dbm.metadata,\n",
    "                                        Column('personal_code_alt', String),\n",
    "                                        extend_existing=True\n",
    "                                       )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zuletzt wird die Datenbasis des TestDataGenerators geändert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_file = dump_dim_data('DT_Personal', 'DT_Personal__neue_personalcode')\n",
    "tdg.change_data({'personal': dump_file})\n",
    "file_path = tdg.dump_data(1000)\n",
    "etl.etl_fact_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}